{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide for the HiggsToBB dataset (Part 2)\n",
    "\n",
    "This notebook is a guide for using the HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC dataset. \n",
    "\n",
    "In Part 2, we focus on machine learning tasks on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Cite as: Duarte, Javier; (2019). Sample with jet, track and secondary vertex properties for Hbb tagging ML studies HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC. CERN Open Data Portal. DOI:[10.7483/OPENDATA.CMS.JGJX.MS7Q](http://doi.org/10.7483/OPENDATA.CMS.JGJX.MS7Q)\n",
    "\n",
    "## Description: \n",
    "\n",
    "The dataset has been produced for developing machine-learning algorithms to differentiate jets originating from a Higgs boson decaying to a bottom quark-antiquark pair (Hbb) from quark or gluon jets originating from quantum chromodynamic (QCD) multijet production.\n",
    "\n",
    "The reconstructed jets are clustered using the anti-kT algorithm with R=0.8 from particle flow (PF) candidates (AK8 jets). The standard L1+L2+L3+residual jet energy corrections are applied to the jets and pileup contamination is mitigated using the charged hadron subtraction (CHS) algorithm. Features of the AK8 jets with transverse momentum pT > 200 GeV and pseudorapidity |η| < 2.4 are provided. Selected features of inclusive (both charged and neutral) PF candidates with pT > 0.95 GeV associated to the AK8 jet are provided. Additional features of charged PF candidates (formed primarily by a charged particle track) with pT > 0.95 GeV associated to the AK8 jet are also provided. Finally, additional features of reconstructed secondary vertices (SVs) associated to the AK8 jet (within ∆R < 0.8) are also provided.\n",
    "\n",
    "## File Information\n",
    "\n",
    "There are two lists of files, one in ROOT format and another in H5. For H5 files, only information for up to 100 particle candidates, up to 60 charged particles or tracks, and up to 5 secondary vertices are stored in zero-padded arrays. \n",
    "\n",
    "List of H5 files: \n",
    "- Test: http://opendata.cern.ch/record/12102/files/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC_test_h5_file_index.txt\n",
    "- Train: http://opendata.cern.ch/record/12102/files/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC_train_h5_file_index.txt\n",
    "\n",
    "List of ROOT files:\n",
    "- Test: http://opendata.cern.ch/record/12102/files/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC_test_root_file_index.txt\n",
    "- Train: http://opendata.cern.ch/record/12102/files/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC_train_root_file_index.txt\n",
    "\n",
    "The size of each H5 file is 1.4 GB, and the size of each ROOT file is 1.1 GB. \n",
    "\n",
    "This notebook help you walk through the **H5** format dataset, and it's derived from this list of notebooks that are developed for the **ROOT** format: https://jmduarte.github.io/capstone-particle-physics-domain/weeks/02-dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tables\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "filename = 'ntuple_merged_11.h5' # train file\n",
    "if not os.path.isfile(filename):\n",
    "    !wget http://opendata.cern.ch/eos/opendata/cms/datascience/HiggsToBBNtupleProducerTool/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC/train/ntuple_merged_11.h5 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Machine Learning Examples  \n",
    "## 2.1 Classification: Particle Identification\n",
    "\n",
    "**Features**: dimension (n,27)\n",
    "\n",
    "**Labels**: dimension (n,2) in one-hot. (1,0) means background(QCD) and (0,1) means H(bb).\n",
    "\n",
    "**Classifiers**:\n",
    "\n",
    "1. Decision Tree Classifier: max depth = 5\n",
    "2. SVM Classifier\n",
    "3. Fully Connected Neural Network Classifier: optimizer='adam', loss='categorical_crossentropy', metrics='accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/yifan/anaconda3/envs/tfvenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/yifan/anaconda3/envs/tfvenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/yifan/anaconda3/envs/tfvenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/yifan/anaconda3/envs/tfvenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/yifan/anaconda3/envs/tfvenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/yifan/anaconda3/envs/tfvenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187912, 27) (187912, 2)\n"
     ]
    }
   ],
   "source": [
    "# selected features useful for the classification task.\n",
    "features = ['fj_jetNTracks', # Number of tracks associated with the AK8 jet\n",
    "            'fj_nSV',  # Number of SVs associated with the AK8 jet (∆R < 0.7)\n",
    "            'fj_tau0_trackEtaRel_0',# Smallest track pseudorapidity ∆η, relative to the jet axis, associated to the 1st N-subjettiness axis\n",
    "            'fj_tau0_trackEtaRel_1', # Second smallest ...\n",
    "            'fj_tau0_trackEtaRel_2', # Third smallest ...\n",
    "            'fj_tau1_trackEtaRel_0', # Smallest track pseudorapidity ∆η, relative to the jet axis, associated to the 2nd N-subjettiness axis\n",
    "            'fj_tau1_trackEtaRel_1', # Second smallest ...\n",
    "            'fj_tau1_trackEtaRel_2', # Thrid smallest ...\n",
    "            'fj_tau_flightDistance2dSig_0', # Transverse (2D) flight distance significance between the PV and the SV with the smallest uncertainty on the 3D flight distance associated to the first N-subjettiness axis\n",
    "            'fj_tau_flightDistance2dSig_1', # ... associated to the second N-subjettiness axis\n",
    "            'fj_tau_vertexDeltaR_0',  # Pseudoangular distance ∆R between the first N-subjettiness axis and SV direction\n",
    "            'fj_tau_vertexEnergyRatio_0', # SV vertex energy ratio for the first N-subjettiness axis, defined as the total energy of all SVs associated with the first N-subjettiness axis divided by the total energy of all the tracks associated with the AK8 jet that are consistent with the PV\n",
    "            'fj_tau_vertexEnergyRatio_1', # SV vertex energy ratio for the second N-subjettiness axis\n",
    "            'fj_tau_vertexMass_0', \n",
    "            'fj_tau_vertexMass_1',\n",
    "            'fj_trackSip2dSigAboveBottom_0',\n",
    "            'fj_trackSip2dSigAboveBottom_1',\n",
    "            'fj_trackSip2dSigAboveCharm_0',\n",
    "            'fj_trackSipdSig_0',\n",
    "            'fj_trackSipdSig_0_0',\n",
    "            'fj_trackSipdSig_0_1',\n",
    "            'fj_trackSipdSig_1',\n",
    "            'fj_trackSipdSig_1_0',\n",
    "            'fj_trackSipdSig_1_1',\n",
    "            'fj_trackSipdSig_2',\n",
    "            'fj_trackSipdSig_3',\n",
    "            'fj_z_ratio']\n",
    "\n",
    "# spectators to define mass/pT window\n",
    "spectators = ['fj_sdmass', # Soft drop mass of the AK8 jet\n",
    "              'fj_pt'] # Transverse momentum of the AK8 jet\n",
    "\n",
    "# 2 labels: QCD or Hbb\n",
    "labels = ['fj_isQCD*sample_isQCD',\n",
    "          'fj_isH*fj_isBB']\n",
    "\n",
    "nfeatures = len(features)\n",
    "nspectators = len(spectators)\n",
    "nlabels = len(labels)\n",
    "\n",
    "def get_features_labels(file_name, remove_mass_pt_window=True):\n",
    "\n",
    "    # load file\n",
    "    h5file = tables.open_file(file_name, 'r')\n",
    "    njets = getattr(h5file.root,features[0]).shape[0]\n",
    "\n",
    "    # allocate arrays\n",
    "    feature_array = np.zeros((njets,nfeatures))\n",
    "    spec_array = np.zeros((njets,nspectators))\n",
    "    label_array = np.zeros((njets,nlabels))\n",
    "\n",
    "    # load feature arrays\n",
    "    for (i, feat) in enumerate(features):\n",
    "        feature_array[:,i] = getattr(h5file.root,feat)[:]\n",
    "\n",
    "    # load spectator arrays\n",
    "    for (i, spec) in enumerate(spectators):\n",
    "        spec_array[:,i] = getattr(h5file.root,spec)[:]\n",
    "\n",
    "    # load labels arrays\n",
    "    for (i, label) in enumerate(labels):\n",
    "        prods = label.split('*')\n",
    "        prod0 = prods[0]\n",
    "        prod1 = prods[1]\n",
    "        fact0 = getattr(h5file.root,prod0)[:]\n",
    "        fact1 = getattr(h5file.root,prod1)[:]\n",
    "        label_array[:,i] = np.multiply(fact0,fact1)\n",
    "\n",
    "    # remove samples outside mass/pT window\n",
    "    if remove_mass_pt_window:\n",
    "        feature_array = feature_array[(spec_array[:,0] > 40) & (spec_array[:,0] < 200) & (spec_array[:,1] > 300) & (spec_array[:,1] < 2000)]\n",
    "        label_array = label_array[(spec_array[:,0] > 40) & (spec_array[:,0] < 200) & (spec_array[:,1] > 300) & (spec_array[:,1] < 2000)]\n",
    "\n",
    "    feature_array = feature_array[np.sum(label_array,axis=1)==1]\n",
    "    label_array = label_array[np.sum(label_array,axis=1)==1]\n",
    "\n",
    "    h5file.close()\n",
    "    return feature_array, label_array\n",
    "\n",
    "# load training file\n",
    "train_filename = 'ntuple_merged_0.h5'\n",
    "feature_array, label_array = get_features_labels(train_filename, remove_mass_pt_window=False)\n",
    "print(feature_array.shape, label_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "clf = clf.fit(feature_array, label_array[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "svm = linear_model.SGDClassifier()\n",
    "svm.fit(feature_array, label_array[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fully Connected Neural Network Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yifan/anaconda3/envs/tfvenv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Define dense keras model:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "bn_1 (BatchNormalization)    (None, 27)                108       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1792      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 5,102\n",
      "Trainable params: 5,048\n",
      "Non-trainable params: 54\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# define dense keras model\n",
    "inputs = Input(shape=(nfeatures,), name = 'input')  \n",
    "x = BatchNormalization(name='bn_1')(inputs)\n",
    "x = Dense(64, name = 'dense_1', activation='relu')(x)\n",
    "x = Dense(32, name = 'dense_2', activation='relu')(x)\n",
    "x = Dense(32, name = 'dense_3', activation='relu')(x)\n",
    "outputs = Dense(nlabels, name = 'output', activation='softmax')(x)\n",
    "keras_model = Model(inputs=inputs, outputs=outputs)\n",
    "keras_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"Define dense keras model:\")\n",
    "print(keras_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train FCNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yifan/anaconda3/envs/tfvenv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 150329 samples, validate on 37583 samples\n",
      "Epoch 1/100\n",
      "150329/150329 [==============================] - 1s 6us/step - loss: 0.2896 - acc: 0.8943 - val_loss: 0.2449 - val_acc: 0.9065\n",
      "Epoch 2/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2407 - acc: 0.9071 - val_loss: 0.2378 - val_acc: 0.9089\n",
      "Epoch 3/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2351 - acc: 0.9089 - val_loss: 0.2340 - val_acc: 0.9104\n",
      "Epoch 4/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2317 - acc: 0.9104 - val_loss: 0.2319 - val_acc: 0.9114\n",
      "Epoch 5/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2292 - acc: 0.9117 - val_loss: 0.2303 - val_acc: 0.9122\n",
      "Epoch 6/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2273 - acc: 0.9127 - val_loss: 0.2290 - val_acc: 0.9132\n",
      "Epoch 7/100\n",
      "150329/150329 [==============================] - 1s 3us/step - loss: 0.2259 - acc: 0.9134 - val_loss: 0.2282 - val_acc: 0.9136\n",
      "Epoch 8/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2246 - acc: 0.9139 - val_loss: 0.2275 - val_acc: 0.9139\n",
      "Epoch 9/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2236 - acc: 0.9144 - val_loss: 0.2271 - val_acc: 0.9142\n",
      "Epoch 10/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2229 - acc: 0.9146 - val_loss: 0.2268 - val_acc: 0.9143\n",
      "Epoch 11/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2222 - acc: 0.9150 - val_loss: 0.2266 - val_acc: 0.9141\n",
      "Epoch 12/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2215 - acc: 0.9152 - val_loss: 0.2263 - val_acc: 0.9138\n",
      "Epoch 13/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2208 - acc: 0.9156 - val_loss: 0.2259 - val_acc: 0.9138\n",
      "Epoch 14/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2203 - acc: 0.9158 - val_loss: 0.2257 - val_acc: 0.9140\n",
      "Epoch 15/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2197 - acc: 0.9160 - val_loss: 0.2254 - val_acc: 0.9143\n",
      "Epoch 16/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2191 - acc: 0.9163 - val_loss: 0.2253 - val_acc: 0.9140\n",
      "Epoch 17/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2187 - acc: 0.9164 - val_loss: 0.2251 - val_acc: 0.9142\n",
      "Epoch 18/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2182 - acc: 0.9166 - val_loss: 0.2251 - val_acc: 0.9142\n",
      "Epoch 19/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2178 - acc: 0.9167 - val_loss: 0.2250 - val_acc: 0.9143\n",
      "Epoch 20/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2174 - acc: 0.9170 - val_loss: 0.2250 - val_acc: 0.9144\n",
      "Epoch 21/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2170 - acc: 0.9171 - val_loss: 0.2248 - val_acc: 0.9148\n",
      "Epoch 22/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2166 - acc: 0.9172 - val_loss: 0.2248 - val_acc: 0.9150\n",
      "Epoch 23/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2162 - acc: 0.9174 - val_loss: 0.2248 - val_acc: 0.9148\n",
      "Epoch 24/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2159 - acc: 0.9175 - val_loss: 0.2248 - val_acc: 0.9147\n",
      "Epoch 25/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2155 - acc: 0.9178 - val_loss: 0.2247 - val_acc: 0.9144\n",
      "Epoch 26/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2152 - acc: 0.9180 - val_loss: 0.2247 - val_acc: 0.9144\n",
      "Epoch 27/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2149 - acc: 0.9181 - val_loss: 0.2248 - val_acc: 0.9145\n",
      "Epoch 28/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2146 - acc: 0.9181 - val_loss: 0.2249 - val_acc: 0.9145\n",
      "Epoch 29/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2143 - acc: 0.9181 - val_loss: 0.2249 - val_acc: 0.9146\n",
      "Epoch 30/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2140 - acc: 0.9184 - val_loss: 0.2249 - val_acc: 0.9144\n",
      "Epoch 31/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2123 - acc: 0.9188 - val_loss: 0.2241 - val_acc: 0.9146\n",
      "Epoch 32/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2119 - acc: 0.9189 - val_loss: 0.2241 - val_acc: 0.9148\n",
      "Epoch 33/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2117 - acc: 0.9192 - val_loss: 0.2241 - val_acc: 0.9147\n",
      "Epoch 34/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2115 - acc: 0.9192 - val_loss: 0.2242 - val_acc: 0.9148\n",
      "Epoch 35/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2113 - acc: 0.9193 - val_loss: 0.2242 - val_acc: 0.9148\n",
      "Epoch 36/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2111 - acc: 0.9194 - val_loss: 0.2243 - val_acc: 0.9146\n",
      "Epoch 37/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2100 - acc: 0.9197 - val_loss: 0.2229 - val_acc: 0.9149\n",
      "Epoch 38/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2098 - acc: 0.9199 - val_loss: 0.2229 - val_acc: 0.9149\n",
      "Epoch 39/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2097 - acc: 0.9200 - val_loss: 0.2229 - val_acc: 0.9148\n",
      "Epoch 40/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2096 - acc: 0.9201 - val_loss: 0.2230 - val_acc: 0.9149\n",
      "Epoch 41/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2095 - acc: 0.9202 - val_loss: 0.2230 - val_acc: 0.9148\n",
      "Epoch 42/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2094 - acc: 0.9203 - val_loss: 0.2231 - val_acc: 0.9148\n",
      "Epoch 43/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2089 - acc: 0.9204 - val_loss: 0.2226 - val_acc: 0.9151\n",
      "Epoch 44/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2087 - acc: 0.9204 - val_loss: 0.2226 - val_acc: 0.9151\n",
      "Epoch 45/100\n",
      "150329/150329 [==============================] - 0s 2us/step - loss: 0.2087 - acc: 0.9203 - val_loss: 0.2227 - val_acc: 0.9151\n",
      "Epoch 46/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2086 - acc: 0.9204 - val_loss: 0.2227 - val_acc: 0.9151\n",
      "Epoch 47/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2086 - acc: 0.9205 - val_loss: 0.2227 - val_acc: 0.9149\n",
      "Epoch 48/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2085 - acc: 0.9205 - val_loss: 0.2227 - val_acc: 0.9149\n",
      "Epoch 49/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2082 - acc: 0.9206 - val_loss: 0.2227 - val_acc: 0.9149\n",
      "Epoch 50/100\n",
      "150329/150329 [==============================] - 1s 3us/step - loss: 0.2082 - acc: 0.9206 - val_loss: 0.2227 - val_acc: 0.9150\n",
      "Epoch 51/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2081 - acc: 0.9206 - val_loss: 0.2227 - val_acc: 0.9150\n",
      "Epoch 52/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2081 - acc: 0.9206 - val_loss: 0.2227 - val_acc: 0.9149\n",
      "Epoch 53/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2081 - acc: 0.9206 - val_loss: 0.2227 - val_acc: 0.9149\n",
      "Epoch 54/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2079 - acc: 0.9207 - val_loss: 0.2227 - val_acc: 0.9150\n",
      "Epoch 55/100\n",
      "150329/150329 [==============================] - 1s 4us/step - loss: 0.2079 - acc: 0.9207 - val_loss: 0.2227 - val_acc: 0.9150\n",
      "Epoch 56/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2079 - acc: 0.9206 - val_loss: 0.2227 - val_acc: 0.9150\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2079 - acc: 0.9207 - val_loss: 0.2227 - val_acc: 0.9150\n",
      "Epoch 58/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2079 - acc: 0.9207 - val_loss: 0.2227 - val_acc: 0.9150\n",
      "Epoch 59/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2078 - acc: 0.9207 - val_loss: 0.2227 - val_acc: 0.9150\n",
      "Epoch 60/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2078 - acc: 0.9207 - val_loss: 0.2227 - val_acc: 0.9150\n",
      "Epoch 61/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2078 - acc: 0.9207 - val_loss: 0.2227 - val_acc: 0.9151\n",
      "Epoch 62/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2077 - acc: 0.9207 - val_loss: 0.2227 - val_acc: 0.9151\n",
      "Epoch 63/100\n",
      "150329/150329 [==============================] - 0s 3us/step - loss: 0.2077 - acc: 0.9207 - val_loss: 0.2227 - val_acc: 0.9151\n"
     ]
    }
   ],
   "source": [
    "# define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(patience=5,factor=0.5)\n",
    "model_checkpoint = ModelCheckpoint('keras_model_best.h5', monitor='val_loss', save_best_only=True)\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "# fit keras model\n",
    "history  = keras_model.fit(feature_array, label_array, batch_size=1024, \n",
    "                epochs=100, validation_split=0.2, shuffle=False,\n",
    "                callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'],label='Loss')\n",
    "plt.plot(history.history['val_loss'],label='Val. loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.title(\"Loss vs Epoch during training on ntuple_merged_0.h5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test and Compare All 3 Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = 'ntuple_merged_11.h5'\n",
    "if not os.path.isfile(test_filename): \n",
    "    !wget http://opendata.cern.ch/eos/opendata/cms/datascience/HiggsToBBNtupleProducerTool/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC/train/ntuple_merged_11.h5 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training file\n",
    "feature_array_test, label_array_test = get_features_labels(test_filename, remove_mass_pt_window=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on file ntuple_merged_11.h5\n",
      "187653/187653 [==============================] - 2s 10us/step\n",
      "Neural network:\n",
      " Test_loss: 0.219699, Test_accuracy: 0.915221\n",
      "Decision Tree Classifier \n",
      " Test_accuracy: 0.902522\n",
      "SVM \n",
      " Test_accuracy: 0.889445\n"
     ]
    }
   ],
   "source": [
    "print(\"Test on file %s\" %test_filename)\n",
    "# run model inference on test data set\n",
    "predict_array_nn = np.argmax(keras_model.predict(feature_array_test),axis=1)\n",
    "predict_array_tree = np.argmax(clf.predict_proba(feature_array_test),axis=1)\n",
    "predict_array_svm = svm.decision_function(feature_array_test)\n",
    "predict_array_svm = (predict_array_svm>0).astype(int)\n",
    "\n",
    "# print test accuracy\n",
    "[nn_test_loss, nn_test_accuracy] = keras_model.evaluate(feature_array_test, label_array_test)\n",
    "print(\"Neural network:\\n Test_loss: %f, Test_accuracy: %f\" %(nn_test_loss, nn_test_accuracy))\n",
    "\n",
    "acc_tree = np.sum((predict_array_tree==label_array_test[:,1]).astype(int))/len(predict_array_tree)\n",
    "print(\"Decision Tree Classifier \\n Test_accuracy: %f\"%acc_tree )\n",
    "\n",
    "acc_svm = np.sum((predict_array_svm==label_array_test[:,1]).astype(int))/len(predict_array_svm)\n",
    "print(\"SVM \\n Test_accuracy: %f\"%acc_svm )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Regression: Energy Reconstruction \n",
    "\n",
    "In energy reconstruction example, the regression target is **fj_gen_pt**. It is the Transverse momentum of the generator-level, geometrically matched heavy particle: H, W, Z, t, etc. (default = -999)\n",
    "\n",
    "**Input**: \n",
    "    \n",
    "    features: dimension (n,27)\n",
    "    \n",
    "**Output**: \n",
    "    \n",
    "    fj_gen_pt: (n,1)\n",
    "    \n",
    "**Model**: \n",
    "    \n",
    "    Fully connected nueral network\n",
    "    \n",
    "In the following example, we use the same set of features as in the classification example to reconstruct the **fj_gen_pt** we target at.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pts = ['fj_gen_pt']\n",
    "\n",
    "def get_features_gen(file_name, remove_mass_pt_window=False):\n",
    "\n",
    "    # load file\n",
    "    h5file = tables.open_file(file_name, 'r')\n",
    "    njets = getattr(h5file.root,features[0]).shape[0]\n",
    "\n",
    "    # allocate arrays\n",
    "    feature_array = np.zeros((njets,nfeatures))\n",
    "    spec_array = np.zeros((njets,nspectators))\n",
    "    label_array = np.zeros((njets,nlabels))\n",
    "    gen_pt_array = np.zeros((njets,1))\n",
    "\n",
    "    # load feature arrays\n",
    "    for (i, feat) in enumerate(features):\n",
    "        feature_array[:,i] = getattr(h5file.root,feat)[:]\n",
    "\n",
    "    for gen in gen_pts:\n",
    "        gen_pt_array[:,0] = getattr(h5file.root,gen)[:]\n",
    "        \n",
    "    # load labels arrays\n",
    "    for (i, label) in enumerate(labels):\n",
    "        prods = label.split('*')\n",
    "        prod0 = prods[0]\n",
    "        prod1 = prods[1]\n",
    "        fact0 = getattr(h5file.root,prod0)[:]\n",
    "        fact1 = getattr(h5file.root,prod1)[:]\n",
    "        label_array[:,i] = np.multiply(fact0,fact1)\n",
    "\n",
    "    feature_array = feature_array[np.sum(label_array,axis=1)==1]\n",
    "    gen_pt_array = gen_pt_array[np.sum(label_array,axis=1)==1]\n",
    "    label_array = label_array[np.sum(label_array,axis=1)==1]\n",
    "\n",
    "    feature_array = feature_array[gen_pt_array[:,0]>=0]\n",
    "    label_array = label_array[gen_pt_array[:,0]>=0]\n",
    "    gen_pt_array = gen_pt_array[gen_pt_array[:,0]>=0]\n",
    "    \n",
    "    h5file.close()\n",
    "    return feature_array, label_array, gen_pt_array\n",
    "\n",
    "\n",
    "# load training file\n",
    "train_filename = 'ntuple_merged_0.h5'\n",
    "feature_array2, label_array2, gen_pt_array = get_features_gen(train_filename, remove_mass_pt_window=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses and Metrics\n",
    "def rmse(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# mean squared error (mse) for regression  (only for Keras tensors)\n",
    "def mse(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "# coefficient of determination (R^2) for regression  (only for Keras tensors)\n",
    "def r_square(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define dense keras model:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "bn_1 (BatchNormalization)    (None, 27)                108       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                1792      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,965\n",
      "Trainable params: 1,911\n",
      "Non-trainable params: 54\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(nfeatures,), name = 'input')  \n",
    "x = BatchNormalization(name='bn_1')(inputs)\n",
    "x = Dense(64, kernel_initializer='he_uniform', activation='relu')(x)\n",
    "outputs = Dense(1, name = 'output')(x)\n",
    "keras_model = Model(inputs=inputs, outputs=outputs)\n",
    "keras_model.compile(optimizer='adam', loss=mse, metrics=[r_square, rmse])\n",
    "print(\"Define dense keras model:\")\n",
    "print(keras_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19824 samples, validate on 4956 samples\n",
      "Epoch 1/300\n",
      "19824/19824 [==============================] - 0s 25us/step - loss: 1264881.6499 - r_square: -4.8171 - rmse: 1023.2984 - val_loss: 1266823.0690 - val_r_square: -4.9133 - val_rmse: 1025.7532\n",
      "Epoch 2/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1261852.5013 - r_square: -4.8031 - rmse: 1021.8276 - val_loss: 1263659.1794 - val_r_square: -4.8986 - val_rmse: 1024.2299\n",
      "Epoch 3/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1258434.1799 - r_square: -4.7874 - rmse: 1020.1701 - val_loss: 1259960.2968 - val_r_square: -4.8813 - val_rmse: 1022.4518\n",
      "Epoch 4/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1254353.6929 - r_square: -4.7686 - rmse: 1018.1949 - val_loss: 1255459.5257 - val_r_square: -4.8603 - val_rmse: 1020.2927\n",
      "Epoch 5/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1249348.2527 - r_square: -4.7456 - rmse: 1015.7760 - val_loss: 1249895.2522 - val_r_square: -4.8343 - val_rmse: 1017.6284\n",
      "Epoch 6/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1243142.8711 - r_square: -4.7170 - rmse: 1012.7807 - val_loss: 1242968.2684 - val_r_square: -4.8020 - val_rmse: 1014.3157\n",
      "Epoch 7/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1235406.6896 - r_square: -4.6814 - rmse: 1009.0481 - val_loss: 1234313.7101 - val_r_square: -4.7616 - val_rmse: 1010.1755\n",
      "Epoch 8/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1225734.4892 - r_square: -4.6369 - rmse: 1004.3784 - val_loss: 1223478.1369 - val_r_square: -4.7110 - val_rmse: 1004.9832\n",
      "Epoch 9/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1213613.3454 - r_square: -4.5811 - rmse: 998.5129 - val_loss: 1209870.0053 - val_r_square: -4.6475 - val_rmse: 998.4393\n",
      "Epoch 10/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1198375.8187 - r_square: -4.5110 - rmse: 991.1077 - val_loss: 1192724.4449 - val_r_square: -4.5675 - val_rmse: 990.1437\n",
      "Epoch 11/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1179144.7457 - r_square: -4.4225 - rmse: 981.6977 - val_loss: 1171045.9799 - val_r_square: -4.4663 - val_rmse: 979.5607\n",
      "Epoch 12/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1154802.5528 - r_square: -4.3105 - rmse: 969.6656 - val_loss: 1143589.7877 - val_r_square: -4.3382 - val_rmse: 965.9876\n",
      "Epoch 13/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 1124081.2582 - r_square: -4.1691 - rmse: 954.2533 - val_loss: 1109093.1889 - val_r_square: -4.1772 - val_rmse: 948.6169\n",
      "Epoch 14/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 1085713.3817 - r_square: -3.9925 - rmse: 934.5976 - val_loss: 1066288.8477 - val_r_square: -3.9774 - val_rmse: 926.5324\n",
      "Epoch 15/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 1038733.9536 - r_square: -3.7764 - rmse: 909.8422 - val_loss: 1014623.5659 - val_r_square: -3.7363 - val_rmse: 898.9901\n",
      "Epoch 16/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 982929.2553 - r_square: -3.5196 - rmse: 879.3406 - val_loss: 954169.3646 - val_r_square: -3.4542 - val_rmse: 865.4201\n",
      "Epoch 17/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 918715.7245 - r_square: -3.2242 - rmse: 842.6410 - val_loss: 885711.4477 - val_r_square: -3.1347 - val_rmse: 825.4815\n",
      "Epoch 18/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 847252.3776 - r_square: -2.8954 - rmse: 799.6749 - val_loss: 810741.0981 - val_r_square: -2.7848 - val_rmse: 779.2119\n",
      "Epoch 19/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 770401.7311 - r_square: -2.5419 - rmse: 750.9530 - val_loss: 731544.8306 - val_r_square: -2.4151 - val_rmse: 727.7491\n",
      "Epoch 20/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 690859.8178 - r_square: -2.1760 - rmse: 698.2203 - val_loss: 651235.1291 - val_r_square: -2.0403 - val_rmse: 673.6933\n",
      "Epoch 21/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 611947.3072 - r_square: -1.8131 - rmse: 644.3519 - val_loss: 573271.1054 - val_r_square: -1.6764 - val_rmse: 619.8718\n",
      "Epoch 22/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 537100.1026 - r_square: -1.4688 - rmse: 592.0441 - val_loss: 500970.1903 - val_r_square: -1.3389 - val_rmse: 569.1183\n",
      "Epoch 23/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 469445.3621 - r_square: -1.1577 - rmse: 543.5656 - val_loss: 437195.7364 - val_r_square: -1.0412 - val_rmse: 523.2298\n",
      "Epoch 24/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 411349.7351 - r_square: -0.8906 - rmse: 500.8611 - val_loss: 383795.7298 - val_r_square: -0.7919 - val_rmse: 483.5141\n",
      "Epoch 25/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 364013.8299 - r_square: -0.6730 - rmse: 466.5154 - val_loss: 341365.8157 - val_r_square: -0.5938 - val_rmse: 452.2070\n",
      "Epoch 26/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 327440.3780 - r_square: -0.5049 - rmse: 441.0080 - val_loss: 309340.3375 - val_r_square: -0.4442 - val_rmse: 429.6099\n",
      "Epoch 27/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 300513.5780 - r_square: -0.3811 - rmse: 423.2959 - val_loss: 286162.9061 - val_r_square: -0.3359 - val_rmse: 414.3091\n",
      "Epoch 28/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 281385.4232 - r_square: -0.2932 - rmse: 411.3286 - val_loss: 269833.9480 - val_r_square: -0.2596 - val_rmse: 403.9129\n",
      "Epoch 29/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 268022.0992 - r_square: -0.2318 - rmse: 403.5788 - val_loss: 258376.3125 - val_r_square: -0.2060 - val_rmse: 396.9247\n",
      "Epoch 30/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 258615.2187 - r_square: -0.1886 - rmse: 398.4319 - val_loss: 250185.4271 - val_r_square: -0.1677 - val_rmse: 392.1967\n",
      "Epoch 31/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 251775.8861 - r_square: -0.1572 - rmse: 394.8277 - val_loss: 244096.7912 - val_r_square: -0.1392 - val_rmse: 388.7106\n",
      "Epoch 32/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 246571.6063 - r_square: -0.1333 - rmse: 392.0838 - val_loss: 239349.4408 - val_r_square: -0.1170 - val_rmse: 385.9603\n",
      "Epoch 33/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 242409.5407 - r_square: -0.1141 - rmse: 389.8228 - val_loss: 235483.9653 - val_r_square: -0.0989 - val_rmse: 383.6302\n",
      "Epoch 34/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 238940.6844 - r_square: -0.0982 - rmse: 387.8457 - val_loss: 232220.8866 - val_r_square: -0.0837 - val_rmse: 381.5600\n",
      "Epoch 35/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 235965.1052 - r_square: -0.0845 - rmse: 386.0561 - val_loss: 229391.9745 - val_r_square: -0.0704 - val_rmse: 379.7000\n",
      "Epoch 36/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 233358.9942 - r_square: -0.0725 - rmse: 384.4226 - val_loss: 226896.4540 - val_r_square: -0.0587 - val_rmse: 378.0315\n",
      "Epoch 37/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 231044.2045 - r_square: -0.0619 - rmse: 382.9255 - val_loss: 224670.9519 - val_r_square: -0.0483 - val_rmse: 376.4985\n",
      "Epoch 38/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 228970.6248 - r_square: -0.0523 - rmse: 381.5420 - val_loss: 222669.6764 - val_r_square: -0.0390 - val_rmse: 375.0838\n",
      "Epoch 39/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 227100.1643 - r_square: -0.0437 - rmse: 380.2654 - val_loss: 220859.1709 - val_r_square: -0.0305 - val_rmse: 373.8257\n",
      "Epoch 40/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 225403.3760 - r_square: -0.0359 - rmse: 379.0973 - val_loss: 219215.0284 - val_r_square: -0.0228 - val_rmse: 372.6880\n",
      "Epoch 41/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 223856.6494 - r_square: -0.0288 - rmse: 378.0268 - val_loss: 217712.9222 - val_r_square: -0.0158 - val_rmse: 371.6415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 222442.8662 - r_square: -0.0223 - rmse: 377.0398 - val_loss: 216335.2309 - val_r_square: -0.0093 - val_rmse: 370.6843\n",
      "Epoch 43/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 221144.7966 - r_square: -0.0163 - rmse: 376.1347 - val_loss: 215069.0489 - val_r_square: -0.0034 - val_rmse: 369.8082\n",
      "Epoch 44/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 219948.6643 - r_square: -0.0108 - rmse: 375.3064 - val_loss: 213899.7148 - val_r_square: 0.0021 - val_rmse: 368.9916\n",
      "Epoch 45/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 218843.8016 - r_square: -0.0057 - rmse: 374.5359 - val_loss: 212818.7993 - val_r_square: 0.0071 - val_rmse: 368.2317\n",
      "Epoch 46/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 217820.6498 - r_square: -0.0010 - rmse: 373.8146 - val_loss: 211818.6226 - val_r_square: 0.0118 - val_rmse: 367.5407\n",
      "Epoch 47/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 216870.9686 - r_square: 0.0033 - rmse: 373.1408 - val_loss: 210889.3515 - val_r_square: 0.0162 - val_rmse: 366.8905\n",
      "Epoch 48/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 215986.4166 - r_square: 0.0074 - rmse: 372.5095 - val_loss: 210024.3909 - val_r_square: 0.0202 - val_rmse: 366.2746\n",
      "Epoch 49/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 215161.1866 - r_square: 0.0112 - rmse: 371.9139 - val_loss: 209218.2767 - val_r_square: 0.0240 - val_rmse: 365.6946\n",
      "Epoch 50/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 214388.4945 - r_square: 0.0148 - rmse: 371.3572 - val_loss: 208463.2322 - val_r_square: 0.0275 - val_rmse: 365.1556\n",
      "Epoch 51/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 213663.2882 - r_square: 0.0181 - rmse: 370.8410 - val_loss: 207755.9869 - val_r_square: 0.0308 - val_rmse: 364.6491\n",
      "Epoch 52/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 212981.2218 - r_square: 0.0212 - rmse: 370.3546 - val_loss: 207093.1497 - val_r_square: 0.0339 - val_rmse: 364.1751\n",
      "Epoch 53/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 212337.9226 - r_square: 0.0242 - rmse: 369.8940 - val_loss: 206469.5591 - val_r_square: 0.0368 - val_rmse: 363.7233\n",
      "Epoch 54/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 211730.4956 - r_square: 0.0270 - rmse: 369.4593 - val_loss: 205882.2047 - val_r_square: 0.0396 - val_rmse: 363.2915\n",
      "Epoch 55/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 211156.4075 - r_square: 0.0296 - rmse: 369.0447 - val_loss: 205328.0229 - val_r_square: 0.0422 - val_rmse: 362.8806\n",
      "Epoch 56/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 210612.5489 - r_square: 0.0321 - rmse: 368.6508 - val_loss: 204804.1012 - val_r_square: 0.0446 - val_rmse: 362.4990\n",
      "Epoch 57/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 210096.6216 - r_square: 0.0345 - rmse: 368.2721 - val_loss: 204308.5592 - val_r_square: 0.0470 - val_rmse: 362.1366\n",
      "Epoch 58/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 209606.2114 - r_square: 0.0368 - rmse: 367.9132 - val_loss: 203839.0445 - val_r_square: 0.0491 - val_rmse: 361.7920\n",
      "Epoch 59/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 209138.8168 - r_square: 0.0389 - rmse: 367.5721 - val_loss: 203393.6116 - val_r_square: 0.0512 - val_rmse: 361.4604\n",
      "Epoch 60/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 208693.2098 - r_square: 0.0410 - rmse: 367.2488 - val_loss: 202970.5516 - val_r_square: 0.0532 - val_rmse: 361.1415\n",
      "Epoch 61/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 208267.4475 - r_square: 0.0429 - rmse: 366.9418 - val_loss: 202567.8504 - val_r_square: 0.0551 - val_rmse: 360.8377\n",
      "Epoch 62/300\n",
      "19824/19824 [==============================] - 0s 4us/step - loss: 207859.8927 - r_square: 0.0448 - rmse: 366.6468 - val_loss: 202183.5938 - val_r_square: 0.0569 - val_rmse: 360.5445\n",
      "Epoch 63/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 207469.1944 - r_square: 0.0466 - rmse: 366.3633 - val_loss: 201817.2758 - val_r_square: 0.0586 - val_rmse: 360.2615\n",
      "Epoch 64/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 207094.5070 - r_square: 0.0483 - rmse: 366.0904 - val_loss: 201467.4413 - val_r_square: 0.0602 - val_rmse: 359.9877\n",
      "Epoch 65/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 206734.9931 - r_square: 0.0500 - rmse: 365.8262 - val_loss: 201133.1035 - val_r_square: 0.0618 - val_rmse: 359.7216\n",
      "Epoch 66/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 206389.7789 - r_square: 0.0515 - rmse: 365.5738 - val_loss: 200813.4198 - val_r_square: 0.0633 - val_rmse: 359.4625\n",
      "Epoch 67/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 206058.3576 - r_square: 0.0531 - rmse: 365.3316 - val_loss: 200507.7246 - val_r_square: 0.0647 - val_rmse: 359.2151\n",
      "Epoch 68/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 205740.0126 - r_square: 0.0545 - rmse: 365.0991 - val_loss: 200215.7922 - val_r_square: 0.0661 - val_rmse: 358.9805\n",
      "Epoch 69/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 205433.8455 - r_square: 0.0559 - rmse: 364.8746 - val_loss: 199936.5787 - val_r_square: 0.0674 - val_rmse: 358.7552\n",
      "Epoch 70/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 205139.3092 - r_square: 0.0573 - rmse: 364.6578 - val_loss: 199669.0977 - val_r_square: 0.0686 - val_rmse: 358.5400\n",
      "Epoch 71/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 204855.7677 - r_square: 0.0586 - rmse: 364.4469 - val_loss: 199412.8049 - val_r_square: 0.0698 - val_rmse: 358.3303\n",
      "Epoch 72/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 204582.4705 - r_square: 0.0599 - rmse: 364.2414 - val_loss: 199166.6338 - val_r_square: 0.0710 - val_rmse: 358.1297\n",
      "Epoch 73/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 204318.8402 - r_square: 0.0611 - rmse: 364.0416 - val_loss: 198929.6698 - val_r_square: 0.0721 - val_rmse: 357.9391\n",
      "Epoch 74/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 204064.5274 - r_square: 0.0622 - rmse: 363.8491 - val_loss: 198701.2119 - val_r_square: 0.0732 - val_rmse: 357.7565\n",
      "Epoch 75/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 203818.4504 - r_square: 0.0634 - rmse: 363.6630 - val_loss: 198481.2830 - val_r_square: 0.0742 - val_rmse: 357.5800\n",
      "Epoch 76/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 203580.3524 - r_square: 0.0645 - rmse: 363.4817 - val_loss: 198270.0309 - val_r_square: 0.0752 - val_rmse: 357.4129\n",
      "Epoch 77/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 203350.2680 - r_square: 0.0655 - rmse: 363.3061 - val_loss: 198066.4950 - val_r_square: 0.0761 - val_rmse: 357.2519\n",
      "Epoch 78/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 203127.2680 - r_square: 0.0666 - rmse: 363.1369 - val_loss: 197870.0585 - val_r_square: 0.0771 - val_rmse: 357.0987\n",
      "Epoch 79/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 202911.3092 - r_square: 0.0675 - rmse: 362.9713 - val_loss: 197680.3817 - val_r_square: 0.0779 - val_rmse: 356.9497\n",
      "Epoch 80/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 202701.8589 - r_square: 0.0685 - rmse: 362.8075 - val_loss: 197497.4916 - val_r_square: 0.0788 - val_rmse: 356.8043\n",
      "Epoch 81/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 202499.1107 - r_square: 0.0694 - rmse: 362.6481 - val_loss: 197320.5260 - val_r_square: 0.0796 - val_rmse: 356.6640\n",
      "Epoch 82/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 202302.6652 - r_square: 0.0703 - rmse: 362.4930 - val_loss: 197150.0759 - val_r_square: 0.0804 - val_rmse: 356.5268\n",
      "Epoch 83/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 202112.0937 - r_square: 0.0712 - rmse: 362.3429 - val_loss: 196985.6585 - val_r_square: 0.0812 - val_rmse: 356.3961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 201927.2309 - r_square: 0.0721 - rmse: 362.1972 - val_loss: 196827.2966 - val_r_square: 0.0819 - val_rmse: 356.2726\n",
      "Epoch 85/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 201747.8334 - r_square: 0.0729 - rmse: 362.0539 - val_loss: 196673.8698 - val_r_square: 0.0826 - val_rmse: 356.1537\n",
      "Epoch 86/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 201573.6224 - r_square: 0.0737 - rmse: 361.9151 - val_loss: 196525.4057 - val_r_square: 0.0833 - val_rmse: 356.0376\n",
      "Epoch 87/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 201404.7546 - r_square: 0.0745 - rmse: 361.7797 - val_loss: 196382.2781 - val_r_square: 0.0840 - val_rmse: 355.9251\n",
      "Epoch 88/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 201240.9084 - r_square: 0.0752 - rmse: 361.6472 - val_loss: 196243.9157 - val_r_square: 0.0846 - val_rmse: 355.8152\n",
      "Epoch 89/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 201081.9347 - r_square: 0.0760 - rmse: 361.5161 - val_loss: 196110.4891 - val_r_square: 0.0853 - val_rmse: 355.7063\n",
      "Epoch 90/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 200927.6296 - r_square: 0.0767 - rmse: 361.3889 - val_loss: 195981.4031 - val_r_square: 0.0859 - val_rmse: 355.6026\n",
      "Epoch 91/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 200777.7890 - r_square: 0.0774 - rmse: 361.2654 - val_loss: 195856.6676 - val_r_square: 0.0865 - val_rmse: 355.5010\n",
      "Epoch 92/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 200631.9528 - r_square: 0.0780 - rmse: 361.1453 - val_loss: 195735.7366 - val_r_square: 0.0870 - val_rmse: 355.4027\n",
      "Epoch 93/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 200490.1547 - r_square: 0.0787 - rmse: 361.0290 - val_loss: 195618.6444 - val_r_square: 0.0876 - val_rmse: 355.3107\n",
      "Epoch 94/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 200352.1972 - r_square: 0.0793 - rmse: 360.9149 - val_loss: 195504.9512 - val_r_square: 0.0881 - val_rmse: 355.2222\n",
      "Epoch 95/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 200218.0249 - r_square: 0.0799 - rmse: 360.8037 - val_loss: 195394.8220 - val_r_square: 0.0886 - val_rmse: 355.1358\n",
      "Epoch 96/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 200087.3644 - r_square: 0.0805 - rmse: 360.6961 - val_loss: 195287.9981 - val_r_square: 0.0891 - val_rmse: 355.0522\n",
      "Epoch 97/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 199959.9598 - r_square: 0.0811 - rmse: 360.5909 - val_loss: 195184.3477 - val_r_square: 0.0896 - val_rmse: 354.9707\n",
      "Epoch 98/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 199835.7821 - r_square: 0.0817 - rmse: 360.4887 - val_loss: 195083.8715 - val_r_square: 0.0901 - val_rmse: 354.8890\n",
      "Epoch 99/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 199714.7289 - r_square: 0.0822 - rmse: 360.3875 - val_loss: 194986.2964 - val_r_square: 0.0905 - val_rmse: 354.8079\n",
      "Epoch 100/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 199596.4531 - r_square: 0.0828 - rmse: 360.2890 - val_loss: 194891.8619 - val_r_square: 0.0910 - val_rmse: 354.7294\n",
      "Epoch 101/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 199480.8883 - r_square: 0.0833 - rmse: 360.1948 - val_loss: 194799.9332 - val_r_square: 0.0914 - val_rmse: 354.6531\n",
      "Epoch 102/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 199368.0128 - r_square: 0.0838 - rmse: 360.1017 - val_loss: 194711.0631 - val_r_square: 0.0918 - val_rmse: 354.5769\n",
      "Epoch 103/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 199257.7408 - r_square: 0.0844 - rmse: 360.0086 - val_loss: 194624.7631 - val_r_square: 0.0922 - val_rmse: 354.5040\n",
      "Epoch 104/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 199149.9662 - r_square: 0.0848 - rmse: 359.9159 - val_loss: 194540.8450 - val_r_square: 0.0926 - val_rmse: 354.4328\n",
      "Epoch 105/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 199044.4438 - r_square: 0.0853 - rmse: 359.8258 - val_loss: 194458.8894 - val_r_square: 0.0930 - val_rmse: 354.3648\n",
      "Epoch 106/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 198941.2485 - r_square: 0.0858 - rmse: 359.7389 - val_loss: 194378.2215 - val_r_square: 0.0934 - val_rmse: 354.3004\n",
      "Epoch 107/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 198840.1656 - r_square: 0.0863 - rmse: 359.6533 - val_loss: 194299.0572 - val_r_square: 0.0937 - val_rmse: 354.2378\n",
      "Epoch 108/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 198741.1032 - r_square: 0.0867 - rmse: 359.5682 - val_loss: 194221.8401 - val_r_square: 0.0941 - val_rmse: 354.1757\n",
      "Epoch 109/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 198643.8399 - r_square: 0.0872 - rmse: 359.4832 - val_loss: 194146.4739 - val_r_square: 0.0944 - val_rmse: 354.1133\n",
      "Epoch 110/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 198548.3369 - r_square: 0.0876 - rmse: 359.3999 - val_loss: 194072.7308 - val_r_square: 0.0948 - val_rmse: 354.0560\n",
      "Epoch 111/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 198454.5213 - r_square: 0.0880 - rmse: 359.3179 - val_loss: 194000.4566 - val_r_square: 0.0951 - val_rmse: 353.9993\n",
      "Epoch 112/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 198362.2693 - r_square: 0.0885 - rmse: 359.2370 - val_loss: 193929.5367 - val_r_square: 0.0955 - val_rmse: 353.9420\n",
      "Epoch 113/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 198271.4656 - r_square: 0.0889 - rmse: 359.1575 - val_loss: 193859.9407 - val_r_square: 0.0958 - val_rmse: 353.8852\n",
      "Epoch 114/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 198182.1720 - r_square: 0.0893 - rmse: 359.0793 - val_loss: 193791.7570 - val_r_square: 0.0961 - val_rmse: 353.8299\n",
      "Epoch 115/300\n",
      "19824/19824 [==============================] - 0s 4us/step - loss: 198094.2850 - r_square: 0.0897 - rmse: 359.0017 - val_loss: 193724.6867 - val_r_square: 0.0964 - val_rmse: 353.7774\n",
      "Epoch 116/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 198007.7727 - r_square: 0.0901 - rmse: 358.9256 - val_loss: 193657.8430 - val_r_square: 0.0967 - val_rmse: 353.7268\n",
      "Epoch 117/300\n",
      "19824/19824 [==============================] - 0s 4us/step - loss: 197922.5798 - r_square: 0.0905 - rmse: 358.8502 - val_loss: 193591.8028 - val_r_square: 0.0970 - val_rmse: 353.6751\n",
      "Epoch 118/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 197838.5285 - r_square: 0.0909 - rmse: 358.7745 - val_loss: 193526.5581 - val_r_square: 0.0973 - val_rmse: 353.6241\n",
      "Epoch 119/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 197755.5211 - r_square: 0.0913 - rmse: 358.6994 - val_loss: 193461.8072 - val_r_square: 0.0976 - val_rmse: 353.5737\n",
      "Epoch 120/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 197673.4952 - r_square: 0.0916 - rmse: 358.6249 - val_loss: 193397.7085 - val_r_square: 0.0979 - val_rmse: 353.5226\n",
      "Epoch 121/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 197592.2556 - r_square: 0.0920 - rmse: 358.5504 - val_loss: 193333.4391 - val_r_square: 0.0982 - val_rmse: 353.4720\n",
      "Epoch 122/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 197511.9127 - r_square: 0.0924 - rmse: 358.4778 - val_loss: 193269.7537 - val_r_square: 0.0985 - val_rmse: 353.4214\n",
      "Epoch 123/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 197432.4218 - r_square: 0.0927 - rmse: 358.4049 - val_loss: 193206.8023 - val_r_square: 0.0988 - val_rmse: 353.3699\n",
      "Epoch 124/300\n",
      "19824/19824 [==============================] - 0s 4us/step - loss: 197353.7072 - r_square: 0.0931 - rmse: 358.3309 - val_loss: 193144.5525 - val_r_square: 0.0991 - val_rmse: 353.3197\n",
      "Epoch 125/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 197275.4545 - r_square: 0.0935 - rmse: 358.2562 - val_loss: 193082.5373 - val_r_square: 0.0994 - val_rmse: 353.2707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 197197.8139 - r_square: 0.0938 - rmse: 358.1823 - val_loss: 193020.8839 - val_r_square: 0.0997 - val_rmse: 353.2216\n",
      "Epoch 127/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 197120.7441 - r_square: 0.0942 - rmse: 358.1087 - val_loss: 192959.5518 - val_r_square: 0.1000 - val_rmse: 353.1729\n",
      "Epoch 128/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 197044.1422 - r_square: 0.0945 - rmse: 358.0353 - val_loss: 192898.1562 - val_r_square: 0.1003 - val_rmse: 353.1234\n",
      "Epoch 129/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196967.7869 - r_square: 0.0949 - rmse: 357.9614 - val_loss: 192836.8223 - val_r_square: 0.1006 - val_rmse: 353.0752\n",
      "Epoch 130/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196891.8841 - r_square: 0.0952 - rmse: 357.8890 - val_loss: 192775.5547 - val_r_square: 0.1008 - val_rmse: 353.0273\n",
      "Epoch 131/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196816.4000 - r_square: 0.0956 - rmse: 357.8154 - val_loss: 192714.3123 - val_r_square: 0.1011 - val_rmse: 352.9779\n",
      "Epoch 132/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 196741.1843 - r_square: 0.0959 - rmse: 357.7408 - val_loss: 192653.0932 - val_r_square: 0.1014 - val_rmse: 352.9292\n",
      "Epoch 133/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196666.0315 - r_square: 0.0963 - rmse: 357.6663 - val_loss: 192591.9619 - val_r_square: 0.1017 - val_rmse: 352.8798\n",
      "Epoch 134/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196590.9202 - r_square: 0.0966 - rmse: 357.5913 - val_loss: 192530.6902 - val_r_square: 0.1020 - val_rmse: 352.8296\n",
      "Epoch 135/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196515.9121 - r_square: 0.0970 - rmse: 357.5163 - val_loss: 192469.3324 - val_r_square: 0.1023 - val_rmse: 352.7793\n",
      "Epoch 136/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196441.0782 - r_square: 0.0973 - rmse: 357.4421 - val_loss: 192407.6195 - val_r_square: 0.1026 - val_rmse: 352.7285\n",
      "Epoch 137/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196366.4892 - r_square: 0.0976 - rmse: 357.3677 - val_loss: 192345.9717 - val_r_square: 0.1029 - val_rmse: 352.6767\n",
      "Epoch 138/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196292.1313 - r_square: 0.0980 - rmse: 357.2934 - val_loss: 192283.8311 - val_r_square: 0.1031 - val_rmse: 352.6253\n",
      "Epoch 139/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196217.8507 - r_square: 0.0983 - rmse: 357.2195 - val_loss: 192220.9969 - val_r_square: 0.1034 - val_rmse: 352.5744\n",
      "Epoch 140/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196143.4871 - r_square: 0.0987 - rmse: 357.1457 - val_loss: 192157.8135 - val_r_square: 0.1037 - val_rmse: 352.5235\n",
      "Epoch 141/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 196069.1379 - r_square: 0.0990 - rmse: 357.0698 - val_loss: 192094.3174 - val_r_square: 0.1040 - val_rmse: 352.4705\n",
      "Epoch 142/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195994.9068 - r_square: 0.0994 - rmse: 356.9937 - val_loss: 192030.7931 - val_r_square: 0.1043 - val_rmse: 352.4165\n",
      "Epoch 143/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195920.6557 - r_square: 0.0997 - rmse: 356.9162 - val_loss: 191966.5804 - val_r_square: 0.1046 - val_rmse: 352.3619\n",
      "Epoch 144/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195846.3669 - r_square: 0.1000 - rmse: 356.8391 - val_loss: 191902.0522 - val_r_square: 0.1049 - val_rmse: 352.3072\n",
      "Epoch 145/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195772.0264 - r_square: 0.1004 - rmse: 356.7626 - val_loss: 191837.3830 - val_r_square: 0.1052 - val_rmse: 352.2520\n",
      "Epoch 146/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195697.5613 - r_square: 0.1007 - rmse: 356.6857 - val_loss: 191772.3444 - val_r_square: 0.1055 - val_rmse: 352.1958\n",
      "Epoch 147/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195622.9505 - r_square: 0.1011 - rmse: 356.6086 - val_loss: 191706.8260 - val_r_square: 0.1058 - val_rmse: 352.1386\n",
      "Epoch 148/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195548.0456 - r_square: 0.1014 - rmse: 356.5307 - val_loss: 191640.5424 - val_r_square: 0.1061 - val_rmse: 352.0798\n",
      "Epoch 149/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195472.7204 - r_square: 0.1018 - rmse: 356.4522 - val_loss: 191573.4877 - val_r_square: 0.1065 - val_rmse: 352.0205\n",
      "Epoch 150/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 195397.1543 - r_square: 0.1021 - rmse: 356.3744 - val_loss: 191506.0718 - val_r_square: 0.1068 - val_rmse: 351.9616\n",
      "Epoch 151/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 195321.4379 - r_square: 0.1025 - rmse: 356.2968 - val_loss: 191438.2502 - val_r_square: 0.1071 - val_rmse: 351.9011\n",
      "Epoch 152/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195245.4738 - r_square: 0.1028 - rmse: 356.2182 - val_loss: 191369.6880 - val_r_square: 0.1074 - val_rmse: 351.8397\n",
      "Epoch 153/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195169.2268 - r_square: 0.1032 - rmse: 356.1399 - val_loss: 191300.1451 - val_r_square: 0.1077 - val_rmse: 351.7769\n",
      "Epoch 154/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195092.5583 - r_square: 0.1035 - rmse: 356.0608 - val_loss: 191229.7940 - val_r_square: 0.1081 - val_rmse: 351.7131\n",
      "Epoch 155/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 195015.4857 - r_square: 0.1039 - rmse: 355.9814 - val_loss: 191158.1784 - val_r_square: 0.1084 - val_rmse: 351.6488\n",
      "Epoch 156/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194937.9540 - r_square: 0.1042 - rmse: 355.9011 - val_loss: 191085.7533 - val_r_square: 0.1087 - val_rmse: 351.5844\n",
      "Epoch 157/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194860.0209 - r_square: 0.1046 - rmse: 355.8209 - val_loss: 191012.7108 - val_r_square: 0.1091 - val_rmse: 351.5184\n",
      "Epoch 158/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194781.6216 - r_square: 0.1049 - rmse: 355.7389 - val_loss: 190938.9172 - val_r_square: 0.1094 - val_rmse: 351.4513\n",
      "Epoch 159/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194702.5149 - r_square: 0.1053 - rmse: 355.6562 - val_loss: 190864.3656 - val_r_square: 0.1098 - val_rmse: 351.3834\n",
      "Epoch 160/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194622.6848 - r_square: 0.1057 - rmse: 355.5735 - val_loss: 190788.8614 - val_r_square: 0.1101 - val_rmse: 351.3147\n",
      "Epoch 161/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194542.2704 - r_square: 0.1060 - rmse: 355.4921 - val_loss: 190712.5292 - val_r_square: 0.1105 - val_rmse: 351.2472\n",
      "Epoch 162/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194461.3873 - r_square: 0.1064 - rmse: 355.4104 - val_loss: 190635.6903 - val_r_square: 0.1108 - val_rmse: 351.1784\n",
      "Epoch 163/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194380.0159 - r_square: 0.1068 - rmse: 355.3276 - val_loss: 190557.4267 - val_r_square: 0.1112 - val_rmse: 351.1087\n",
      "Epoch 164/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194298.1036 - r_square: 0.1072 - rmse: 355.2449 - val_loss: 190478.3106 - val_r_square: 0.1116 - val_rmse: 351.0379\n",
      "Epoch 165/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194215.6945 - r_square: 0.1075 - rmse: 355.1617 - val_loss: 190398.5033 - val_r_square: 0.1120 - val_rmse: 350.9669\n",
      "Epoch 166/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 194132.6910 - r_square: 0.1079 - rmse: 355.0792 - val_loss: 190318.4212 - val_r_square: 0.1123 - val_rmse: 350.8965\n",
      "Epoch 167/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19824/19824 [==============================] - 0s 2us/step - loss: 194049.1710 - r_square: 0.1083 - rmse: 354.9953 - val_loss: 190236.9945 - val_r_square: 0.1127 - val_rmse: 350.8237\n",
      "Epoch 168/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193965.1992 - r_square: 0.1087 - rmse: 354.9097 - val_loss: 190154.3103 - val_r_square: 0.1131 - val_rmse: 350.7479\n",
      "Epoch 169/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193880.5047 - r_square: 0.1091 - rmse: 354.8227 - val_loss: 190070.6148 - val_r_square: 0.1135 - val_rmse: 350.6721\n",
      "Epoch 170/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193795.1938 - r_square: 0.1095 - rmse: 354.7375 - val_loss: 189985.7776 - val_r_square: 0.1139 - val_rmse: 350.5952\n",
      "Epoch 171/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193709.2067 - r_square: 0.1099 - rmse: 354.6494 - val_loss: 189899.6088 - val_r_square: 0.1143 - val_rmse: 350.5138\n",
      "Epoch 172/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193622.5257 - r_square: 0.1103 - rmse: 354.5605 - val_loss: 189812.4715 - val_r_square: 0.1147 - val_rmse: 350.4325\n",
      "Epoch 173/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193535.3467 - r_square: 0.1107 - rmse: 354.4712 - val_loss: 189724.3774 - val_r_square: 0.1151 - val_rmse: 350.3500\n",
      "Epoch 174/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193447.5777 - r_square: 0.1111 - rmse: 354.3819 - val_loss: 189635.0704 - val_r_square: 0.1155 - val_rmse: 350.2676\n",
      "Epoch 175/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193359.1781 - r_square: 0.1115 - rmse: 354.2922 - val_loss: 189545.0366 - val_r_square: 0.1159 - val_rmse: 350.1835\n",
      "Epoch 176/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193270.1334 - r_square: 0.1119 - rmse: 354.1993 - val_loss: 189453.8521 - val_r_square: 0.1164 - val_rmse: 350.0967\n",
      "Epoch 177/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193180.4471 - r_square: 0.1123 - rmse: 354.1052 - val_loss: 189361.7499 - val_r_square: 0.1168 - val_rmse: 350.0092\n",
      "Epoch 178/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 193090.3444 - r_square: 0.1127 - rmse: 354.0121 - val_loss: 189268.9201 - val_r_square: 0.1172 - val_rmse: 349.9216\n",
      "Epoch 179/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192999.7684 - r_square: 0.1131 - rmse: 353.9178 - val_loss: 189175.1868 - val_r_square: 0.1177 - val_rmse: 349.8357\n",
      "Epoch 180/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192908.5772 - r_square: 0.1136 - rmse: 353.8229 - val_loss: 189080.2877 - val_r_square: 0.1181 - val_rmse: 349.7499\n",
      "Epoch 181/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192816.9198 - r_square: 0.1140 - rmse: 353.7278 - val_loss: 188984.3716 - val_r_square: 0.1186 - val_rmse: 349.6626\n",
      "Epoch 182/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192724.7353 - r_square: 0.1144 - rmse: 353.6315 - val_loss: 188887.6005 - val_r_square: 0.1190 - val_rmse: 349.5734\n",
      "Epoch 183/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192631.9345 - r_square: 0.1148 - rmse: 353.5348 - val_loss: 188789.5359 - val_r_square: 0.1195 - val_rmse: 349.4835\n",
      "Epoch 184/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192538.5371 - r_square: 0.1153 - rmse: 353.4381 - val_loss: 188690.1945 - val_r_square: 0.1199 - val_rmse: 349.3913\n",
      "Epoch 185/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192444.2721 - r_square: 0.1157 - rmse: 353.3394 - val_loss: 188589.6599 - val_r_square: 0.1204 - val_rmse: 349.2989\n",
      "Epoch 186/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192349.4426 - r_square: 0.1161 - rmse: 353.2408 - val_loss: 188487.7649 - val_r_square: 0.1209 - val_rmse: 349.2057\n",
      "Epoch 187/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192254.0960 - r_square: 0.1166 - rmse: 353.1408 - val_loss: 188384.8136 - val_r_square: 0.1214 - val_rmse: 349.1121\n",
      "Epoch 188/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192158.3372 - r_square: 0.1170 - rmse: 353.0411 - val_loss: 188281.0451 - val_r_square: 0.1218 - val_rmse: 349.0180\n",
      "Epoch 189/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 192062.0919 - r_square: 0.1174 - rmse: 352.9404 - val_loss: 188176.3949 - val_r_square: 0.1223 - val_rmse: 348.9209\n",
      "Epoch 190/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 191965.1284 - r_square: 0.1179 - rmse: 352.8383 - val_loss: 188070.5064 - val_r_square: 0.1228 - val_rmse: 348.8228\n",
      "Epoch 191/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 191867.4637 - r_square: 0.1183 - rmse: 352.7369 - val_loss: 187963.5202 - val_r_square: 0.1233 - val_rmse: 348.7245\n",
      "Epoch 192/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 191769.3903 - r_square: 0.1188 - rmse: 352.6329 - val_loss: 187855.6166 - val_r_square: 0.1238 - val_rmse: 348.6224\n",
      "Epoch 193/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 191670.9124 - r_square: 0.1192 - rmse: 352.5286 - val_loss: 187747.1739 - val_r_square: 0.1243 - val_rmse: 348.5197\n",
      "Epoch 194/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 191572.0661 - r_square: 0.1197 - rmse: 352.4253 - val_loss: 187637.8149 - val_r_square: 0.1248 - val_rmse: 348.4168\n",
      "Epoch 195/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 191473.0221 - r_square: 0.1202 - rmse: 352.3226 - val_loss: 187527.6292 - val_r_square: 0.1254 - val_rmse: 348.3123\n",
      "Epoch 196/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 191373.6412 - r_square: 0.1206 - rmse: 352.2186 - val_loss: 187416.4337 - val_r_square: 0.1259 - val_rmse: 348.2066\n",
      "Epoch 197/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 191274.0530 - r_square: 0.1211 - rmse: 352.1133 - val_loss: 187304.9627 - val_r_square: 0.1264 - val_rmse: 348.0997\n",
      "Epoch 198/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 191174.1829 - r_square: 0.1215 - rmse: 352.0071 - val_loss: 187192.4948 - val_r_square: 0.1269 - val_rmse: 347.9931\n",
      "Epoch 199/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 191074.0850 - r_square: 0.1220 - rmse: 351.9017 - val_loss: 187079.3801 - val_r_square: 0.1275 - val_rmse: 347.8854\n",
      "Epoch 200/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 190973.6960 - r_square: 0.1224 - rmse: 351.7951 - val_loss: 186965.4269 - val_r_square: 0.1280 - val_rmse: 347.7769\n",
      "Epoch 201/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 190873.1348 - r_square: 0.1229 - rmse: 351.6879 - val_loss: 186851.2092 - val_r_square: 0.1285 - val_rmse: 347.6694\n",
      "Epoch 202/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 190772.5486 - r_square: 0.1234 - rmse: 351.5806 - val_loss: 186736.3839 - val_r_square: 0.1291 - val_rmse: 347.5665\n",
      "Epoch 203/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 190672.1149 - r_square: 0.1238 - rmse: 351.4749 - val_loss: 186621.6695 - val_r_square: 0.1296 - val_rmse: 347.4627\n",
      "Epoch 204/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 190571.7360 - r_square: 0.1243 - rmse: 351.3670 - val_loss: 186506.3832 - val_r_square: 0.1301 - val_rmse: 347.3572\n",
      "Epoch 205/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 190471.3854 - r_square: 0.1248 - rmse: 351.2583 - val_loss: 186390.3932 - val_r_square: 0.1307 - val_rmse: 347.2512\n",
      "Epoch 206/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 190371.0827 - r_square: 0.1252 - rmse: 351.1490 - val_loss: 186274.5965 - val_r_square: 0.1312 - val_rmse: 347.1455\n",
      "Epoch 207/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 190271.1311 - r_square: 0.1257 - rmse: 351.0416 - val_loss: 186158.5657 - val_r_square: 0.1318 - val_rmse: 347.0406\n",
      "Epoch 208/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 190171.6914 - r_square: 0.1261 - rmse: 350.9338 - val_loss: 186042.5347 - val_r_square: 0.1323 - val_rmse: 346.9341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 190072.7418 - r_square: 0.1266 - rmse: 350.8265 - val_loss: 185926.4245 - val_r_square: 0.1328 - val_rmse: 346.8260\n",
      "Epoch 210/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 189974.2783 - r_square: 0.1270 - rmse: 350.7204 - val_loss: 185811.2544 - val_r_square: 0.1334 - val_rmse: 346.7181\n",
      "Epoch 211/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 189876.3116 - r_square: 0.1275 - rmse: 350.6148 - val_loss: 185696.1027 - val_r_square: 0.1339 - val_rmse: 346.6103\n",
      "Epoch 212/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 189778.7309 - r_square: 0.1279 - rmse: 350.5102 - val_loss: 185582.0919 - val_r_square: 0.1344 - val_rmse: 346.5049\n",
      "Epoch 213/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 189681.5231 - r_square: 0.1284 - rmse: 350.4068 - val_loss: 185467.4597 - val_r_square: 0.1350 - val_rmse: 346.4004\n",
      "Epoch 214/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 189584.9454 - r_square: 0.1288 - rmse: 350.3043 - val_loss: 185353.4846 - val_r_square: 0.1355 - val_rmse: 346.2956\n",
      "Epoch 215/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 189488.9480 - r_square: 0.1293 - rmse: 350.2009 - val_loss: 185239.7197 - val_r_square: 0.1360 - val_rmse: 346.1916\n",
      "Epoch 216/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 189393.6259 - r_square: 0.1297 - rmse: 350.0968 - val_loss: 185126.6315 - val_r_square: 0.1366 - val_rmse: 346.0867\n",
      "Epoch 217/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 189298.6823 - r_square: 0.1302 - rmse: 349.9916 - val_loss: 185014.3211 - val_r_square: 0.1371 - val_rmse: 345.9837\n",
      "Epoch 218/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 189204.1257 - r_square: 0.1306 - rmse: 349.8880 - val_loss: 184902.5775 - val_r_square: 0.1376 - val_rmse: 345.8813\n",
      "Epoch 219/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 189110.0218 - r_square: 0.1310 - rmse: 349.7855 - val_loss: 184791.8682 - val_r_square: 0.1381 - val_rmse: 345.7782\n",
      "Epoch 220/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 189016.4762 - r_square: 0.1315 - rmse: 349.6842 - val_loss: 184681.5170 - val_r_square: 0.1387 - val_rmse: 345.6755\n",
      "Epoch 221/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188923.4755 - r_square: 0.1319 - rmse: 349.5842 - val_loss: 184571.1348 - val_r_square: 0.1392 - val_rmse: 345.5722\n",
      "Epoch 222/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188831.0749 - r_square: 0.1323 - rmse: 349.4826 - val_loss: 184461.6296 - val_r_square: 0.1397 - val_rmse: 345.4676\n",
      "Epoch 223/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188739.3692 - r_square: 0.1327 - rmse: 349.3823 - val_loss: 184352.9825 - val_r_square: 0.1402 - val_rmse: 345.3653\n",
      "Epoch 224/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188648.2554 - r_square: 0.1331 - rmse: 349.2811 - val_loss: 184244.8391 - val_r_square: 0.1407 - val_rmse: 345.2622\n",
      "Epoch 225/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188558.0213 - r_square: 0.1336 - rmse: 349.1819 - val_loss: 184137.5138 - val_r_square: 0.1412 - val_rmse: 345.1620\n",
      "Epoch 226/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188468.8978 - r_square: 0.1340 - rmse: 349.0855 - val_loss: 184032.6642 - val_r_square: 0.1417 - val_rmse: 345.0614\n",
      "Epoch 227/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188380.3453 - r_square: 0.1344 - rmse: 348.9876 - val_loss: 183928.7679 - val_r_square: 0.1422 - val_rmse: 344.9603\n",
      "Epoch 228/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188292.5237 - r_square: 0.1348 - rmse: 348.8905 - val_loss: 183825.7664 - val_r_square: 0.1426 - val_rmse: 344.8615\n",
      "Epoch 229/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188205.6203 - r_square: 0.1352 - rmse: 348.7955 - val_loss: 183724.1864 - val_r_square: 0.1431 - val_rmse: 344.7645\n",
      "Epoch 230/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188120.0026 - r_square: 0.1356 - rmse: 348.7027 - val_loss: 183623.7640 - val_r_square: 0.1436 - val_rmse: 344.6674\n",
      "Epoch 231/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 188035.4103 - r_square: 0.1360 - rmse: 348.6103 - val_loss: 183524.3831 - val_r_square: 0.1441 - val_rmse: 344.5695\n",
      "Epoch 232/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187951.6926 - r_square: 0.1363 - rmse: 348.5178 - val_loss: 183426.3620 - val_r_square: 0.1445 - val_rmse: 344.4733\n",
      "Epoch 233/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187869.1317 - r_square: 0.1367 - rmse: 348.4256 - val_loss: 183329.3597 - val_r_square: 0.1450 - val_rmse: 344.3770\n",
      "Epoch 234/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187787.5845 - r_square: 0.1371 - rmse: 348.3335 - val_loss: 183232.7229 - val_r_square: 0.1454 - val_rmse: 344.2818\n",
      "Epoch 235/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187707.0792 - r_square: 0.1375 - rmse: 348.2427 - val_loss: 183137.4102 - val_r_square: 0.1459 - val_rmse: 344.1869\n",
      "Epoch 236/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187627.5971 - r_square: 0.1378 - rmse: 348.1531 - val_loss: 183043.3739 - val_r_square: 0.1463 - val_rmse: 344.0930\n",
      "Epoch 237/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187549.2954 - r_square: 0.1382 - rmse: 348.0627 - val_loss: 182951.2036 - val_r_square: 0.1467 - val_rmse: 343.9978\n",
      "Epoch 238/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 187471.7368 - r_square: 0.1386 - rmse: 347.9736 - val_loss: 182860.7613 - val_r_square: 0.1472 - val_rmse: 343.9067\n",
      "Epoch 239/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187395.5155 - r_square: 0.1389 - rmse: 347.8860 - val_loss: 182772.4094 - val_r_square: 0.1476 - val_rmse: 343.8143\n",
      "Epoch 240/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187320.3179 - r_square: 0.1393 - rmse: 347.7990 - val_loss: 182685.1689 - val_r_square: 0.1480 - val_rmse: 343.7252\n",
      "Epoch 241/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187246.5069 - r_square: 0.1396 - rmse: 347.7143 - val_loss: 182599.5739 - val_r_square: 0.1484 - val_rmse: 343.6404\n",
      "Epoch 242/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187174.3674 - r_square: 0.1399 - rmse: 347.6342 - val_loss: 182515.7775 - val_r_square: 0.1488 - val_rmse: 343.5583\n",
      "Epoch 243/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187103.6950 - r_square: 0.1402 - rmse: 347.5533 - val_loss: 182433.5258 - val_r_square: 0.1491 - val_rmse: 343.4739\n",
      "Epoch 244/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 187033.9319 - r_square: 0.1406 - rmse: 347.4728 - val_loss: 182352.7009 - val_r_square: 0.1495 - val_rmse: 343.3901\n",
      "Epoch 245/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186965.0774 - r_square: 0.1409 - rmse: 347.3915 - val_loss: 182273.3344 - val_r_square: 0.1499 - val_rmse: 343.3073\n",
      "Epoch 246/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186897.2751 - r_square: 0.1412 - rmse: 347.3122 - val_loss: 182195.6662 - val_r_square: 0.1503 - val_rmse: 343.2253\n",
      "Epoch 247/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186830.4169 - r_square: 0.1415 - rmse: 347.2316 - val_loss: 182119.3548 - val_r_square: 0.1506 - val_rmse: 343.1420\n",
      "Epoch 248/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186764.2546 - r_square: 0.1418 - rmse: 347.1533 - val_loss: 182044.2554 - val_r_square: 0.1510 - val_rmse: 343.0612\n",
      "Epoch 249/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186699.0960 - r_square: 0.1421 - rmse: 347.0764 - val_loss: 181971.2630 - val_r_square: 0.1513 - val_rmse: 342.9816\n",
      "Epoch 250/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19824/19824 [==============================] - 0s 2us/step - loss: 186635.0802 - r_square: 0.1424 - rmse: 347.0004 - val_loss: 181898.4956 - val_r_square: 0.1516 - val_rmse: 342.9013\n",
      "Epoch 251/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186571.9869 - r_square: 0.1427 - rmse: 346.9248 - val_loss: 181827.3447 - val_r_square: 0.1520 - val_rmse: 342.8228\n",
      "Epoch 252/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186509.5729 - r_square: 0.1430 - rmse: 346.8495 - val_loss: 181757.8229 - val_r_square: 0.1523 - val_rmse: 342.7447\n",
      "Epoch 253/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186447.8966 - r_square: 0.1433 - rmse: 346.7745 - val_loss: 181689.7163 - val_r_square: 0.1526 - val_rmse: 342.6681\n",
      "Epoch 254/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186387.0474 - r_square: 0.1435 - rmse: 346.6996 - val_loss: 181623.1730 - val_r_square: 0.1529 - val_rmse: 342.5905\n",
      "Epoch 255/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186326.9773 - r_square: 0.1438 - rmse: 346.6257 - val_loss: 181557.0689 - val_r_square: 0.1532 - val_rmse: 342.5160\n",
      "Epoch 256/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186268.0661 - r_square: 0.1441 - rmse: 346.5534 - val_loss: 181491.4306 - val_r_square: 0.1535 - val_rmse: 342.4405\n",
      "Epoch 257/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186210.0515 - r_square: 0.1444 - rmse: 346.4823 - val_loss: 181427.2086 - val_r_square: 0.1538 - val_rmse: 342.3679\n",
      "Epoch 258/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186153.0833 - r_square: 0.1446 - rmse: 346.4127 - val_loss: 181364.4960 - val_r_square: 0.1541 - val_rmse: 342.2939\n",
      "Epoch 259/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186096.7692 - r_square: 0.1449 - rmse: 346.3417 - val_loss: 181302.9469 - val_r_square: 0.1544 - val_rmse: 342.2209\n",
      "Epoch 260/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 186041.0984 - r_square: 0.1451 - rmse: 346.2725 - val_loss: 181242.6862 - val_r_square: 0.1547 - val_rmse: 342.1503\n",
      "Epoch 261/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185986.0130 - r_square: 0.1454 - rmse: 346.2039 - val_loss: 181183.1063 - val_r_square: 0.1550 - val_rmse: 342.0805\n",
      "Epoch 262/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 185931.8386 - r_square: 0.1456 - rmse: 346.1371 - val_loss: 181124.7029 - val_r_square: 0.1553 - val_rmse: 342.0103\n",
      "Epoch 263/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185878.4637 - r_square: 0.1459 - rmse: 346.0694 - val_loss: 181068.0367 - val_r_square: 0.1555 - val_rmse: 341.9408\n",
      "Epoch 264/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185825.5693 - r_square: 0.1461 - rmse: 346.0035 - val_loss: 181011.9916 - val_r_square: 0.1558 - val_rmse: 341.8730\n",
      "Epoch 265/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185773.2971 - r_square: 0.1464 - rmse: 345.9374 - val_loss: 180957.5926 - val_r_square: 0.1560 - val_rmse: 341.8050\n",
      "Epoch 266/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185721.4571 - r_square: 0.1466 - rmse: 345.8736 - val_loss: 180903.5664 - val_r_square: 0.1563 - val_rmse: 341.7395\n",
      "Epoch 267/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185670.4660 - r_square: 0.1468 - rmse: 345.8101 - val_loss: 180850.8668 - val_r_square: 0.1565 - val_rmse: 341.6724\n",
      "Epoch 268/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185620.0143 - r_square: 0.1471 - rmse: 345.7469 - val_loss: 180798.3083 - val_r_square: 0.1568 - val_rmse: 341.6068\n",
      "Epoch 269/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 185570.1903 - r_square: 0.1473 - rmse: 345.6848 - val_loss: 180746.5735 - val_r_square: 0.1570 - val_rmse: 341.5434\n",
      "Epoch 270/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185520.7234 - r_square: 0.1475 - rmse: 345.6219 - val_loss: 180695.5758 - val_r_square: 0.1573 - val_rmse: 341.4803\n",
      "Epoch 271/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185471.7551 - r_square: 0.1477 - rmse: 345.5601 - val_loss: 180644.7591 - val_r_square: 0.1575 - val_rmse: 341.4175\n",
      "Epoch 272/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185423.5120 - r_square: 0.1480 - rmse: 345.5000 - val_loss: 180594.5066 - val_r_square: 0.1577 - val_rmse: 341.3553\n",
      "Epoch 273/300\n",
      "19824/19824 [==============================] - 0s 3us/step - loss: 185376.0359 - r_square: 0.1482 - rmse: 345.4388 - val_loss: 180545.5396 - val_r_square: 0.1580 - val_rmse: 341.2920\n",
      "Epoch 274/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185328.7562 - r_square: 0.1484 - rmse: 345.3790 - val_loss: 180497.9300 - val_r_square: 0.1582 - val_rmse: 341.2316\n",
      "Epoch 275/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185281.9345 - r_square: 0.1486 - rmse: 345.3212 - val_loss: 180451.3103 - val_r_square: 0.1584 - val_rmse: 341.1731\n",
      "Epoch 276/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185235.7583 - r_square: 0.1488 - rmse: 345.2637 - val_loss: 180405.7411 - val_r_square: 0.1586 - val_rmse: 341.1147\n",
      "Epoch 277/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185190.0250 - r_square: 0.1490 - rmse: 345.2072 - val_loss: 180359.8653 - val_r_square: 0.1588 - val_rmse: 341.0578\n",
      "Epoch 278/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185144.9470 - r_square: 0.1493 - rmse: 345.1519 - val_loss: 180315.2213 - val_r_square: 0.1590 - val_rmse: 341.0005\n",
      "Epoch 279/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185100.3927 - r_square: 0.1495 - rmse: 345.0965 - val_loss: 180271.4029 - val_r_square: 0.1592 - val_rmse: 340.9448\n",
      "Epoch 280/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185056.2397 - r_square: 0.1497 - rmse: 345.0427 - val_loss: 180228.8133 - val_r_square: 0.1594 - val_rmse: 340.8909\n",
      "Epoch 281/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 185012.3099 - r_square: 0.1499 - rmse: 344.9881 - val_loss: 180187.2630 - val_r_square: 0.1596 - val_rmse: 340.8378\n",
      "Epoch 282/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184968.7590 - r_square: 0.1501 - rmse: 344.9358 - val_loss: 180145.9200 - val_r_square: 0.1598 - val_rmse: 340.7839\n",
      "Epoch 283/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184925.4196 - r_square: 0.1503 - rmse: 344.8811 - val_loss: 180105.5638 - val_r_square: 0.1600 - val_rmse: 340.7299\n",
      "Epoch 284/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184882.4460 - r_square: 0.1505 - rmse: 344.8303 - val_loss: 180064.6254 - val_r_square: 0.1602 - val_rmse: 340.6779\n",
      "Epoch 285/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184840.3085 - r_square: 0.1507 - rmse: 344.7799 - val_loss: 180024.4968 - val_r_square: 0.1604 - val_rmse: 340.6247\n",
      "Epoch 286/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184798.4271 - r_square: 0.1508 - rmse: 344.7304 - val_loss: 179985.0163 - val_r_square: 0.1606 - val_rmse: 340.5733\n",
      "Epoch 287/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184757.2901 - r_square: 0.1510 - rmse: 344.6817 - val_loss: 179946.4657 - val_r_square: 0.1608 - val_rmse: 340.5214\n",
      "Epoch 288/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184716.3841 - r_square: 0.1512 - rmse: 344.6325 - val_loss: 179908.8064 - val_r_square: 0.1609 - val_rmse: 340.4705\n",
      "Epoch 289/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184675.9789 - r_square: 0.1514 - rmse: 344.5842 - val_loss: 179871.4190 - val_r_square: 0.1611 - val_rmse: 340.4203\n",
      "Epoch 290/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184635.8233 - r_square: 0.1516 - rmse: 344.5382 - val_loss: 179834.0986 - val_r_square: 0.1613 - val_rmse: 340.3710\n",
      "Epoch 291/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184595.7804 - r_square: 0.1518 - rmse: 344.4900 - val_loss: 179797.0818 - val_r_square: 0.1615 - val_rmse: 340.3207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 292/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184555.8277 - r_square: 0.1520 - rmse: 344.4433 - val_loss: 179760.8578 - val_r_square: 0.1616 - val_rmse: 340.2712\n",
      "Epoch 293/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184515.9922 - r_square: 0.1521 - rmse: 344.3956 - val_loss: 179724.3786 - val_r_square: 0.1618 - val_rmse: 340.2202\n",
      "Epoch 294/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184476.1262 - r_square: 0.1523 - rmse: 344.3480 - val_loss: 179686.4839 - val_r_square: 0.1620 - val_rmse: 340.1712\n",
      "Epoch 295/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184436.5783 - r_square: 0.1525 - rmse: 344.3028 - val_loss: 179649.2909 - val_r_square: 0.1621 - val_rmse: 340.1203\n",
      "Epoch 296/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184397.0899 - r_square: 0.1527 - rmse: 344.2563 - val_loss: 179611.8187 - val_r_square: 0.1623 - val_rmse: 340.0709\n",
      "Epoch 297/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184357.8062 - r_square: 0.1529 - rmse: 344.2098 - val_loss: 179574.8697 - val_r_square: 0.1625 - val_rmse: 340.0196\n",
      "Epoch 298/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184318.5620 - r_square: 0.1530 - rmse: 344.1631 - val_loss: 179538.4202 - val_r_square: 0.1627 - val_rmse: 339.9690\n",
      "Epoch 299/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184279.2882 - r_square: 0.1532 - rmse: 344.1168 - val_loss: 179502.3362 - val_r_square: 0.1628 - val_rmse: 339.9195\n",
      "Epoch 300/300\n",
      "19824/19824 [==============================] - 0s 2us/step - loss: 184240.2819 - r_square: 0.1534 - rmse: 344.0720 - val_loss: 179466.7076 - val_r_square: 0.1630 - val_rmse: 339.8706\n"
     ]
    }
   ],
   "source": [
    "# define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(patience=5,factor=0.5)\n",
    "model_checkpoint = ModelCheckpoint('keras_regression_best.h5', monitor='val_loss', save_best_only=True)\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "# fit keras model\n",
    "history  = keras_model.fit(feature_array2, gen_pt_array, batch_size=1024, \n",
    "                epochs=300, validation_split=0.2, shuffle=False,\n",
    "                callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'],label='Loss')\n",
    "plt.plot(history.history['val_loss'],label='Val. loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.title(\"Loss vs Epoch during training on ntuple_merged_0.h5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Evaluate \n",
    "# load training file\n",
    "test_filename = 'ntuple_merged_0.h5'\n",
    "feature_array_test2, label_array_test2, gen_pt_array_test = get_features_gen(test_filename, remove_mass_pt_window=False)\n",
    "predict_array_nn = keras_model.predict(feature_array_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24780/24780 [==============================] - 0s 11us/step\n",
      "Neural network:\n",
      " Test_loss: 183829.230307, Test_r_square: 0.119075, Test_rmse: 343.150444\n"
     ]
    }
   ],
   "source": [
    "[nn_test_mse, nn_test_rsquare, nn_test_rmse] = keras_model.evaluate(feature_array_test2, gen_pt_array_test)\n",
    "print(\"Neural network:\\n Test_loss: %f, Test_r_square: %f, Test_rmse: %f\" %(nn_test_mse, nn_test_rsquare, nn_test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfvenv",
   "language": "python",
   "name": "tfvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
